{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUoPMjo-gByH"
      },
      "source": [
        "# 1. Import required libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0y-BUyW7fbnF"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import io "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sY8V5yldgE6l"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "warnings.simplefilter(action='ignore', category=UserWarning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0hPVyJMBgHIE",
        "outputId": "2a059c95-30de-4ab8-e8aa-fc7af9b3df12"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FcvAlnxigMxo"
      },
      "source": [
        "# 2. Import Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GsN4hspOgI8a"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/Datasets/dataset_2_2.csv', encoding='latin-1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r1txOc8ZgRuA",
        "outputId": "54acef90-86fc-4014-bd6f-5cdac27a5f60"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-169cf02d-737a-4a98-bb8b-14a730f5fa1e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Location</th>\n",
              "      <th>Text</th>\n",
              "      <th>Sentiment</th>\n",
              "      <th>Likes</th>\n",
              "      <th>quotes</th>\n",
              "      <th>replies</th>\n",
              "      <th>retweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>NewYork</td>\n",
              "      <td>Woke criminal New York Times brings back Covid...</td>\n",
              "      <td>Negative</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>NewYork</td>\n",
              "      <td>He proven New York liberal narcissist hired wo...</td>\n",
              "      <td>Negative</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>NewYork</td>\n",
              "      <td>CBS2 News At 11 New York leader say mask vacci...</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>NewYork</td>\n",
              "      <td>Pediatric COVID Deaths New York</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>NewYork</td>\n",
              "      <td>The 7 day rolling average new Covid hospitaliz...</td>\n",
              "      <td>Negative</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-169cf02d-737a-4a98-bb8b-14a730f5fa1e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-169cf02d-737a-4a98-bb8b-14a730f5fa1e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-169cf02d-737a-4a98-bb8b-14a730f5fa1e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   Unnamed: 0 Location                                               Text  \\\n",
              "0           0  NewYork  Woke criminal New York Times brings back Covid...   \n",
              "1           1  NewYork  He proven New York liberal narcissist hired wo...   \n",
              "2           2  NewYork  CBS2 News At 11 New York leader say mask vacci...   \n",
              "3           3  NewYork                   Pediatric COVID Deaths New York    \n",
              "4           4  NewYork  The 7 day rolling average new Covid hospitaliz...   \n",
              "\n",
              "  Sentiment  Likes  quotes  replies  retweet  \n",
              "0  Negative    1.0     0.0      1.0      0.0  \n",
              "1  Negative    1.0     0.0      0.0      0.0  \n",
              "2   Neutral    5.0     0.0      0.0      3.0  \n",
              "3   Neutral    3.0     0.0      0.0      1.0  \n",
              "4  Negative    2.0     0.0      0.0      0.0  "
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9iNhBG4gVVG"
      },
      "source": [
        "# 3. Dataset Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vx3kLV-bgUYa"
      },
      "outputs": [],
      "source": [
        "df.drop(['Unnamed: 0'], axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "94BGc6BNgZ-s"
      },
      "outputs": [],
      "source": [
        "sent_dict = {\n",
        "    'Neutral':1,\n",
        "    'Negative':0,\n",
        "    'Positive':2\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XOoTdA_GgcEp"
      },
      "outputs": [],
      "source": [
        "df = df[['Text','Sentiment', 'Location']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iTungSYIgd6c",
        "outputId": "d1405cd6-92d8-408e-dd55-5e0621ed7c66"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Neutral     144366\n",
              "Negative    132071\n",
              "Positive    113166\n",
              "Name: Sentiment, dtype: int64"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['Sentiment'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Hg3oLXiggiz"
      },
      "outputs": [],
      "source": [
        "df['Sentiment'] = df['Sentiment'].map(sent_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lu0qWEdDgkOY",
        "outputId": "9261ef7d-ca46-4ce7-d9af-7a2570407262"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-bd631bbf-6035-46c0-9062-4282e00f40c9\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text</th>\n",
              "      <th>Sentiment</th>\n",
              "      <th>Location</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Woke criminal New York Times brings back Covid...</td>\n",
              "      <td>0</td>\n",
              "      <td>NewYork</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>He proven New York liberal narcissist hired wo...</td>\n",
              "      <td>0</td>\n",
              "      <td>NewYork</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>CBS2 News At 11 New York leader say mask vacci...</td>\n",
              "      <td>1</td>\n",
              "      <td>NewYork</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Pediatric COVID Deaths New York</td>\n",
              "      <td>1</td>\n",
              "      <td>NewYork</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The 7 day rolling average new Covid hospitaliz...</td>\n",
              "      <td>0</td>\n",
              "      <td>NewYork</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bd631bbf-6035-46c0-9062-4282e00f40c9')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-bd631bbf-6035-46c0-9062-4282e00f40c9 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-bd631bbf-6035-46c0-9062-4282e00f40c9');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                                Text  Sentiment Location\n",
              "0  Woke criminal New York Times brings back Covid...          0  NewYork\n",
              "1  He proven New York liberal narcissist hired wo...          0  NewYork\n",
              "2  CBS2 News At 11 New York leader say mask vacci...          1  NewYork\n",
              "3                   Pediatric COVID Deaths New York           1  NewYork\n",
              "4  The 7 day rolling average new Covid hospitaliz...          0  NewYork"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x1oXOnrIgrnP"
      },
      "outputs": [],
      "source": [
        "df = df.rename(columns = {\"Sentiment\": \"labels\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VNwHd7B1qMcO"
      },
      "outputs": [],
      "source": [
        "df.dropna(inplace=True)\n",
        "df.reset_index(inplace=True, drop=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwjTag4mhDtZ"
      },
      "source": [
        "# 4. Separate according to region - datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U5Y2OBsKiuyn"
      },
      "outputs": [],
      "source": [
        "location_dataset = []\n",
        "locations = list(df['Location'].unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-3M17DcpTVR8"
      },
      "outputs": [],
      "source": [
        "locations = locations[:6]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ied4w4SjjZIk"
      },
      "outputs": [],
      "source": [
        "for location in locations:\n",
        "  data = df[df['Location']==location]\n",
        "  data = data.iloc[:10000,:]\n",
        "  location_dataset.append(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s90c7B7HkH1I",
        "outputId": "6b0b4d99-f14c-4843-d537-6f107989ecf2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "<class 'pandas.core.frame.DataFrame'>\n"
          ]
        }
      ],
      "source": [
        "for location in location_dataset:\n",
        "  print(type(location))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_rinTVQgwaw"
      },
      "source": [
        "# 5. Train-test split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l2MSDAZ8g19z"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oacZstmekAwr"
      },
      "outputs": [],
      "source": [
        "train_texts_list = []\n",
        "train_labels_list = []\n",
        "test_texts_list = []\n",
        "test_labels_list = []\n",
        "\n",
        "for location in location_dataset:\n",
        "  train_texts, test_texts, train_labels, test_labels = train_test_split(location['Text'], location['labels'], test_size=.2)\n",
        "\n",
        "  train_texts_list.append(train_texts)\n",
        "  test_texts_list.append(test_texts)\n",
        "  train_labels_list.append(train_labels)\n",
        "  test_labels_list.append(test_labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "maoQjtxGl-gq"
      },
      "outputs": [],
      "source": [
        "def reseting_index(df):\n",
        "  return df.reset_index(inplace=True, drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "On0MvUILmHok"
      },
      "outputs": [],
      "source": [
        "for df in train_texts_list:\n",
        "  reseting_index(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AQXnHNWVmcPl"
      },
      "outputs": [],
      "source": [
        "for df in train_labels_list:\n",
        "  reseting_index(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AW5AojLUmxLZ"
      },
      "outputs": [],
      "source": [
        "common_df = []\n",
        "for df in test_texts_list:\n",
        "  common_df = common_df + list(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7X33jk4anAUf"
      },
      "outputs": [],
      "source": [
        "common_df_labels = []\n",
        "for df in test_labels_list:\n",
        "  common_df_labels = common_df_labels + list(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "txt-cd8onKRO"
      },
      "outputs": [],
      "source": [
        "test_texts = pd.DataFrame({'Text':common_df})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l5SbWt7cnouN"
      },
      "outputs": [],
      "source": [
        "test_labels = pd.DataFrame({'labels':common_df_labels})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omE2sWHKn82F"
      },
      "source": [
        "# 6. Import tokenizer and model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4VUuWRkmnq71",
        "outputId": "82636b55-878c-46aa-825b-377920bc779c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.18.0-py3-none-any.whl (4.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 4.3 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 65.5 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.5.1-py3-none-any.whl (77 kB)\n",
            "\u001b[K     |████████████████████████████████| 77 kB 8.9 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 74.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.11.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.5 MB 59.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.63.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.5.1 pyyaml-6.0 sacremoses-0.0.49 tokenizers-0.11.6 transformers-4.18.0\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A3hG3mpYoAsN",
        "outputId": "e9b51a45-0b89-41a8-adf6-2a00b5902656"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a532a14e82c84e5faadc3fdab7804118",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "341c86eb8da54d8da84194161efc5f61",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4eddd91f533e4b13a2547e02318ed072",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "13d4bf984f294bd792b8808bc167e8c8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/455k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d56aab34d2454367a7f9114fadb7ca40",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/420M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModel,AutoTokenizer,TFAutoModel\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = AutoModel.from_pretrained('bert-base-uncased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eFJCAmsMoKMR"
      },
      "outputs": [],
      "source": [
        "def tokenize_location_datasets(train_texts_list):\n",
        "  tokenized_train_texts_list = []\n",
        "\n",
        "  for df in train_texts_list:\n",
        "    tokenized_train_texts_list.append(tokenizer(list(df), truncation=True, max_length=256, padding=True))\n",
        "  \n",
        "  return tokenized_train_texts_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dpm_lvs_pNsR"
      },
      "outputs": [],
      "source": [
        "test_encodings = tokenizer(test_texts['Text'].tolist(), truncation=True, max_length=256, padding=True)\n",
        "train_encodings_list = tokenize_location_datasets(train_texts_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvilxQ9pq9NQ"
      },
      "source": [
        "# 7. Setup for pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pgzcGEXyqqEY"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "class TweetsDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, label):\n",
        "        self.encodings = encodings\n",
        "        self.label = label\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['label'] = torch.tensor(self.label[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.label)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4DJy8xH6rHoN"
      },
      "outputs": [],
      "source": [
        "def gen_train_datasets(train_encodings_list, train_labels_list):\n",
        "  train_datasets = []\n",
        "\n",
        "  for i, train_encs in enumerate(train_encodings_list):\n",
        "    labels = train_labels_list[i]\n",
        "    train_dataset = TweetsDataset(train_encs, labels)\n",
        "    train_datasets.append(train_dataset)\n",
        "\n",
        "  return train_datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qiReFlcCrcbU"
      },
      "outputs": [],
      "source": [
        "train_datasets = gen_train_datasets(train_encodings_list, train_labels_list)\n",
        "test_dataset = TweetsDataset(test_encodings, test_labels['labels'].tolist())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hcg_OwOqsXvC"
      },
      "source": [
        "# 8. Adapt Pre-trained BERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WsJDAKRer91o"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "from sklearn.utils.class_weight import compute_class_weight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U-CpoW_FshuU"
      },
      "outputs": [],
      "source": [
        "class CustomBERTModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CustomBERTModel, self).__init__()\n",
        "        self.num_labels = 3\n",
        "\n",
        "        self.bert = AutoModel.from_pretrained('bert-base-uncased')\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.classifier = nn.Linear(768, 3)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        position_ids=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        labels=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n",
        "            Labels for computing the sequence classification/regression loss. Indices should be in :obj:`[0, ...,\n",
        "            config.num_labels - 1]`. If :obj:`config.num_labels == 1` a regression loss is computed (Mean-Square loss),\n",
        "            If :obj:`config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
        "        \"\"\"\n",
        "\n",
        "        outputs = self.bert(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "\n",
        "        pooled_output = outputs[1]\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        logits = self.classifier(pooled_output)\n",
        "\n",
        "        class_weights = compute_class_weight('balanced', classes = np.unique(train_labels), y = train_labels)\n",
        "        weights = torch.tensor(class_weights, dtype = torch.float)\n",
        "        weights = weights.cuda()\n",
        "\n",
        "        loss_fct = nn.CrossEntropyLoss(weight=weights)\n",
        "        loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "        output = (logits,) + outputs[2:]\n",
        "        return ((loss,) + output) if loss is not None else output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qI1hKJ6LssRX"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from transformers import AdamW\n",
        "import transformers\n",
        "import math\n",
        "import copy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCQFuyq0tpln"
      },
      "source": [
        "# 9. Functions for average and training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aC8aUqRPtvXv"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "batch_size = 4\n",
        "cycles = 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WHHDsLuCt0Ja"
      },
      "outputs": [],
      "source": [
        "def create_n_models(train_datasets):\n",
        "  num = len(train_datasets)\n",
        "\n",
        "  models = []\n",
        "\n",
        "  for i in range(num):\n",
        "    model = CustomBERTModel()\n",
        "    model = model.cuda()\n",
        "    model.to(device)\n",
        "\n",
        "    train_loader = DataLoader(train_datasets[i], batch_size=batch_size, shuffle=True)\n",
        "    optim = AdamW(model.parameters(), lr=5e-5,  weight_decay=0.01)\n",
        "    warm_steps = int(len(train_datasets[i])*0.1/batch_size*cycles)\n",
        "    train_steps = int(len(train_datasets[i])/batch_size*cycles)\n",
        "    scheduler = transformers.get_linear_schedule_with_warmup(optim, warm_steps, train_steps)\n",
        "\n",
        "    models.append({'model': model, 'loader': train_loader, 'optim': optim, 'scheduler': scheduler, 'n_samples': len(train_datasets[i])})\n",
        "\n",
        "  return models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z66gZY0nyOB0"
      },
      "outputs": [],
      "source": [
        "def fedavg(models):\n",
        "  params1 = models[0]['model'].named_parameters()\n",
        "  resulting_params = models[0]['model'].named_parameters()\n",
        "  resulting_params = copy.deepcopy(dict(resulting_params))\n",
        "\n",
        "  for name, _ in params1:\n",
        "    total = 0\n",
        "\n",
        "    for model_dict in models:\n",
        "      model = model_dict['model']\n",
        "      total += dict(model.named_parameters())[name]\n",
        "    \n",
        "    total = total/len(train_datasets)\n",
        "    resulting_params[name].data.copy_(total)\n",
        "\n",
        "  model = CustomBERTModel()\n",
        "  model = model.cuda()\n",
        "  model.to(device)\n",
        "  model.load_state_dict(resulting_params, strict=False)\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MZj29q-HuY5O"
      },
      "outputs": [],
      "source": [
        "def same2(m1, m2):\n",
        "  m2_dict = dict(m2.named_parameters())\n",
        "  for n, p in m1.named_parameters():\n",
        "    if not torch.all(torch.eq(p, m2_dict[n])):\n",
        "      return False\n",
        "  return True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "USV7GCxZ4fAm"
      },
      "outputs": [],
      "source": [
        "def update_models(global_model, models):\n",
        "\n",
        "  for i in range(len(models)):\n",
        "\n",
        "    m = models[i]['model']\n",
        "\n",
        "    m.load_state_dict(copy.deepcopy(dict(global_model.named_parameters())), strict=False)\n",
        "    models[i]['model'] = m\n",
        " \n",
        "  return models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oupNxfZrFRjJ"
      },
      "source": [
        "# 10. Evaluation code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZD4PaA7gH3iT"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "from sklearn.metrics import classification_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "psGJM4DgH7mO"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(pred):\n",
        "    label = pred['label_ids']\n",
        "    preds = pred['predictions'].argmax(-1)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(label, preds, average='macro')\n",
        "    acc = accuracy_score(label, preds)\n",
        "    cr = classification_report(label, preds, digits=3)\n",
        "    print(acc)\n",
        "    return {\n",
        "        'accuracy': acc,\n",
        "        'f1': f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3fnMKDV5IN9S"
      },
      "outputs": [],
      "source": [
        "def evaluate(model):\n",
        "  test_loader = DataLoader(test_dataset, batch_size=16, shuffle=True)\n",
        "  pred = {\n",
        "          'label_ids': torch.empty(0).to(device),\n",
        "          'predictions':torch.empty(0).to(device)\n",
        "          }\n",
        "  model.eval()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for s, batch in enumerate(test_loader):\n",
        "      \n",
        "      if s %300 == 0 and not s == 0:\n",
        "        print('Batch {:>5,} of {:>5,}'.format(s, len(test_loader)))\n",
        "\n",
        "      input_ids = batch['input_ids'].to(device)\n",
        "      attention_mask = batch['attention_mask'].to(device)\n",
        "      label = batch['label'].to(device)\n",
        "\n",
        "      outputs = model(input_ids, attention_mask=attention_mask, labels=label)\n",
        "\n",
        "      pred['predictions'] = torch.cat((pred['predictions'], outputs[1]), 0)\n",
        "      pred['label_ids'] = torch.cat((pred['label_ids'], label), 0)\n",
        "\n",
        "  pred['predictions'] = pred['predictions'].cpu()\n",
        "  pred['label_ids'] = pred['label_ids'].cpu()\n",
        "\n",
        "  compute_metrics(pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9ZTD8hdIyQR"
      },
      "source": [
        "# 11. Training code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c6fQ-t4GI48t"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "start_time = time.time()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yo3RNmDHyXYM"
      },
      "outputs": [],
      "source": [
        "model_list = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SgQ5vWwZI764"
      },
      "outputs": [],
      "source": [
        "def train(cycles, epochs):\n",
        "  models = create_n_models(train_datasets)\n",
        "  model_list.append(models)\n",
        "\n",
        "  for cycle in range(cycles):\n",
        "    print('---------- Starting cycle {} ----------'.format(cycle+1))\n",
        "\n",
        "    for j, model_dict in enumerate(models):\n",
        "      print('---------- Training model No: {} ----------'.format(j+1))\n",
        "\n",
        "      model = model_dict['model']\n",
        "      test = copy.deepcopy(model_dict['model'])\n",
        "\n",
        "      model.train()\n",
        "\n",
        "      train_loader = model_dict['loader']\n",
        "      optim = model_dict['optim']\n",
        "      scheduler = model_dict['scheduler']\n",
        "\n",
        "      for epoch in range(epochs):\n",
        "        print('---------- Epoch No: {} ----------'.format(epoch+1))\n",
        "\n",
        "        for s, batch in enumerate(train_loader):\n",
        "\n",
        "          if s %300 == 0 and not s == 0:\n",
        "            print('Batch {:>5,} of {:>5,}'.format(s, len(train_loader)))\n",
        "\n",
        "          optim.zero_grad()\n",
        "\n",
        "          input_ids = batch['input_ids'].to(device)\n",
        "          attention_mask = batch['attention_mask'].to(device)\n",
        "          label = batch['label'].to(device)\n",
        "\n",
        "          outputs = model(input_ids, attention_mask=attention_mask, labels=label)\n",
        "          loss = outputs[0]\n",
        "\n",
        "          loss.backward()\n",
        "          optim.step()\n",
        "          scheduler.step()\n",
        "\n",
        "    for i,m in enumerate(models):\n",
        "      print('---------- Evaluating model No: {} ----------'.format(i+1))\n",
        "      evaluate(m['model'])\n",
        "\n",
        "    print('---------- Evaluating global model ----------')\n",
        "    global_model = fedavg(models)\n",
        "    evaluate(global_model)\n",
        "    models = update_models(global_model, models)\n",
        "\n",
        "    model_list.append(models)\n",
        "\n",
        "    del global_model\n",
        "\n",
        "  return models\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "F84lU3GQMU7Q",
        "outputId": "d6103cba-e6cc-4a53-b32a-5cb3e8b1753b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------- Starting cycle 1 ----------\n",
            "---------- Training model No: 1 ----------\n",
            "---------- Epoch No: 1 ----------\n",
            "Batch   300 of 2,000\n",
            "Batch   600 of 2,000\n",
            "Batch   900 of 2,000\n",
            "Batch 1,200 of 2,000\n",
            "Batch 1,500 of 2,000\n",
            "Batch 1,800 of 2,000\n",
            "---------- Epoch No: 2 ----------\n",
            "Batch   300 of 2,000\n",
            "Batch   600 of 2,000\n",
            "Batch   900 of 2,000\n",
            "Batch 1,200 of 2,000\n",
            "Batch 1,500 of 2,000\n",
            "Batch 1,800 of 2,000\n",
            "---------- Epoch No: 3 ----------\n",
            "Batch   300 of 2,000\n",
            "Batch   600 of 2,000\n",
            "Batch   900 of 2,000\n",
            "Batch 1,200 of 2,000\n",
            "Batch 1,500 of 2,000\n",
            "Batch 1,800 of 2,000\n",
            "---------- Epoch No: 4 ----------\n",
            "Batch   300 of 2,000\n",
            "Batch   600 of 2,000\n",
            "Batch   900 of 2,000\n",
            "Batch 1,200 of 2,000\n",
            "Batch 1,500 of 2,000\n",
            "Batch 1,800 of 2,000\n",
            "---------- Epoch No: 5 ----------\n",
            "Batch   300 of 2,000\n",
            "Batch   600 of 2,000\n",
            "Batch   900 of 2,000\n",
            "Batch 1,200 of 2,000\n",
            "Batch 1,500 of 2,000\n",
            "Batch 1,800 of 2,000\n",
            "---------- Training model No: 2 ----------\n",
            "---------- Epoch No: 1 ----------\n",
            "Batch   300 of 2,000\n",
            "Batch   600 of 2,000\n",
            "Batch   900 of 2,000\n",
            "Batch 1,200 of 2,000\n",
            "Batch 1,500 of 2,000\n",
            "Batch 1,800 of 2,000\n",
            "---------- Epoch No: 2 ----------\n",
            "Batch   300 of 2,000\n",
            "Batch   600 of 2,000\n",
            "Batch   900 of 2,000\n",
            "Batch 1,200 of 2,000\n",
            "Batch 1,500 of 2,000\n",
            "Batch 1,800 of 2,000\n",
            "---------- Epoch No: 3 ----------\n",
            "Batch   300 of 2,000\n",
            "Batch   600 of 2,000\n",
            "Batch   900 of 2,000\n",
            "Batch 1,200 of 2,000\n",
            "Batch 1,500 of 2,000\n",
            "Batch 1,800 of 2,000\n",
            "---------- Epoch No: 4 ----------\n",
            "Batch   300 of 2,000\n",
            "Batch   600 of 2,000\n",
            "Batch   900 of 2,000\n",
            "Batch 1,200 of 2,000\n",
            "Batch 1,500 of 2,000\n",
            "Batch 1,800 of 2,000\n",
            "---------- Epoch No: 5 ----------\n",
            "Batch   300 of 2,000\n",
            "Batch   600 of 2,000\n",
            "Batch   900 of 2,000\n",
            "Batch 1,200 of 2,000\n",
            "Batch 1,500 of 2,000\n",
            "Batch 1,800 of 2,000\n",
            "---------- Training model No: 3 ----------\n",
            "---------- Epoch No: 1 ----------\n",
            "Batch   300 of 2,000\n",
            "Batch   600 of 2,000\n",
            "Batch   900 of 2,000\n",
            "Batch 1,200 of 2,000\n",
            "Batch 1,500 of 2,000\n",
            "Batch 1,800 of 2,000\n",
            "---------- Epoch No: 2 ----------\n",
            "Batch   300 of 2,000\n",
            "Batch   600 of 2,000\n",
            "Batch   900 of 2,000\n",
            "Batch 1,200 of 2,000\n",
            "Batch 1,500 of 2,000\n",
            "Batch 1,800 of 2,000\n",
            "---------- Epoch No: 3 ----------\n",
            "Batch   300 of 2,000\n",
            "Batch   600 of 2,000\n",
            "Batch   900 of 2,000\n",
            "Batch 1,200 of 2,000\n",
            "Batch 1,500 of 2,000\n",
            "Batch 1,800 of 2,000\n",
            "---------- Epoch No: 4 ----------\n",
            "Batch   300 of 2,000\n",
            "Batch   600 of 2,000\n",
            "Batch   900 of 2,000\n",
            "Batch 1,200 of 2,000\n",
            "Batch 1,500 of 2,000\n",
            "Batch 1,800 of 2,000\n",
            "---------- Epoch No: 5 ----------\n",
            "Batch   300 of 2,000\n",
            "Batch   600 of 2,000\n",
            "Batch   900 of 2,000\n",
            "Batch 1,200 of 2,000\n",
            "Batch 1,500 of 2,000\n",
            "Batch 1,800 of 2,000\n",
            "---------- Training model No: 4 ----------\n",
            "---------- Epoch No: 1 ----------\n",
            "Batch   300 of 2,000\n",
            "Batch   600 of 2,000\n",
            "Batch   900 of 2,000\n",
            "Batch 1,200 of 2,000\n",
            "Batch 1,500 of 2,000\n",
            "Batch 1,800 of 2,000\n",
            "---------- Epoch No: 2 ----------\n",
            "Batch   300 of 2,000\n",
            "Batch   600 of 2,000\n",
            "Batch   900 of 2,000\n",
            "Batch 1,200 of 2,000\n",
            "Batch 1,500 of 2,000\n",
            "Batch 1,800 of 2,000\n",
            "---------- Epoch No: 3 ----------\n",
            "Batch   300 of 2,000\n",
            "Batch   600 of 2,000\n",
            "Batch   900 of 2,000\n",
            "Batch 1,200 of 2,000\n",
            "Batch 1,500 of 2,000\n",
            "Batch 1,800 of 2,000\n",
            "---------- Epoch No: 4 ----------\n",
            "Batch   300 of 2,000\n",
            "Batch   600 of 2,000\n",
            "Batch   900 of 2,000\n",
            "Batch 1,200 of 2,000\n",
            "Batch 1,500 of 2,000\n",
            "Batch 1,800 of 2,000\n",
            "---------- Epoch No: 5 ----------\n",
            "Batch   300 of 2,000\n",
            "Batch   600 of 2,000\n",
            "Batch   900 of 2,000\n",
            "Batch 1,200 of 2,000\n",
            "Batch 1,500 of 2,000\n",
            "Batch 1,800 of 2,000\n",
            "---------- Training model No: 5 ----------\n",
            "---------- Epoch No: 1 ----------\n",
            "Batch   300 of 2,000\n",
            "Batch   600 of 2,000\n",
            "Batch   900 of 2,000\n",
            "Batch 1,200 of 2,000\n",
            "Batch 1,500 of 2,000\n",
            "Batch 1,800 of 2,000\n",
            "---------- Epoch No: 2 ----------\n",
            "Batch   300 of 2,000\n",
            "Batch   600 of 2,000\n",
            "Batch   900 of 2,000\n",
            "Batch 1,200 of 2,000\n",
            "Batch 1,500 of 2,000\n",
            "Batch 1,800 of 2,000\n",
            "---------- Epoch No: 3 ----------\n",
            "Batch   300 of 2,000\n",
            "Batch   600 of 2,000\n",
            "Batch   900 of 2,000\n",
            "Batch 1,200 of 2,000\n",
            "Batch 1,500 of 2,000\n",
            "Batch 1,800 of 2,000\n",
            "---------- Epoch No: 4 ----------\n",
            "Batch   300 of 2,000\n",
            "Batch   600 of 2,000\n",
            "Batch   900 of 2,000\n",
            "Batch 1,200 of 2,000\n",
            "Batch 1,500 of 2,000\n",
            "Batch 1,800 of 2,000\n",
            "---------- Epoch No: 5 ----------\n",
            "Batch   300 of 2,000\n",
            "Batch   600 of 2,000\n",
            "Batch   900 of 2,000\n",
            "Batch 1,200 of 2,000\n",
            "Batch 1,500 of 2,000\n",
            "Batch 1,800 of 2,000\n",
            "---------- Training model No: 6 ----------\n",
            "---------- Epoch No: 1 ----------\n",
            "Batch   300 of 2,000\n",
            "Batch   600 of 2,000\n",
            "Batch   900 of 2,000\n",
            "Batch 1,200 of 2,000\n",
            "Batch 1,500 of 2,000\n",
            "Batch 1,800 of 2,000\n",
            "---------- Epoch No: 2 ----------\n",
            "Batch   300 of 2,000\n",
            "Batch   600 of 2,000\n",
            "Batch   900 of 2,000\n",
            "Batch 1,200 of 2,000\n",
            "Batch 1,500 of 2,000\n",
            "Batch 1,800 of 2,000\n",
            "---------- Epoch No: 3 ----------\n",
            "Batch   300 of 2,000\n",
            "Batch   600 of 2,000\n",
            "Batch   900 of 2,000\n",
            "Batch 1,200 of 2,000\n",
            "Batch 1,500 of 2,000\n",
            "Batch 1,800 of 2,000\n",
            "---------- Epoch No: 4 ----------\n",
            "Batch   300 of 2,000\n",
            "Batch   600 of 2,000\n",
            "Batch   900 of 2,000\n",
            "Batch 1,200 of 2,000\n",
            "Batch 1,500 of 2,000\n",
            "Batch 1,800 of 2,000\n",
            "---------- Epoch No: 5 ----------\n",
            "Batch   300 of 2,000\n",
            "Batch   600 of 2,000\n",
            "Batch   900 of 2,000\n",
            "Batch 1,200 of 2,000\n",
            "Batch 1,500 of 2,000\n",
            "Batch 1,800 of 2,000\n",
            "---------- Evaluating model No: 1 ----------\n",
            "Batch   300 of   750\n",
            "Batch   600 of   750\n",
            "0.84225\n",
            "---------- Evaluating model No: 2 ----------\n",
            "Batch   300 of   750\n",
            "Batch   600 of   750\n",
            "0.8485833333333334\n",
            "---------- Evaluating model No: 3 ----------\n",
            "Batch   300 of   750\n",
            "Batch   600 of   750\n",
            "0.847\n",
            "---------- Evaluating model No: 4 ----------\n",
            "Batch   300 of   750\n",
            "Batch   600 of   750\n",
            "0.839\n",
            "---------- Evaluating model No: 5 ----------\n",
            "Batch   300 of   750\n",
            "Batch   600 of   750\n",
            "0.8610833333333333\n",
            "---------- Evaluating model No: 6 ----------\n",
            "Batch   300 of   750\n",
            "Batch   600 of   750\n",
            "0.8544166666666667\n",
            "---------- Evaluating global model ----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch   300 of   750\n",
            "Batch   600 of   750\n",
            "0.869\n",
            "---------- Starting cycle 2 ----------\n",
            "---------- Training model No: 1 ----------\n",
            "---------- Epoch No: 1 ----------\n",
            "Batch   300 of 2,000\n",
            "Batch   600 of 2,000\n",
            "Batch   900 of 2,000\n",
            "Batch 1,200 of 2,000\n",
            "Batch 1,500 of 2,000\n",
            "Batch 1,800 of 2,000\n",
            "---------- Epoch No: 2 ----------\n",
            "Batch   300 of 2,000\n",
            "Batch   600 of 2,000\n",
            "Batch   900 of 2,000\n",
            "Batch 1,200 of 2,000\n",
            "Batch 1,500 of 2,000\n",
            "Batch 1,800 of 2,000\n",
            "---------- Epoch No: 3 ----------\n",
            "Batch   300 of 2,000\n",
            "Batch   600 of 2,000\n",
            "Batch   900 of 2,000\n",
            "Batch 1,200 of 2,000\n",
            "Batch 1,500 of 2,000\n",
            "Batch 1,800 of 2,000\n",
            "---------- Epoch No: 4 ----------\n",
            "Batch   300 of 2,000\n",
            "Batch   600 of 2,000\n",
            "Batch   900 of 2,000\n",
            "Batch 1,200 of 2,000\n",
            "Batch 1,500 of 2,000\n",
            "Batch 1,800 of 2,000\n",
            "---------- Epoch No: 5 ----------\n",
            "Batch   300 of 2,000\n",
            "Batch   600 of 2,000\n",
            "Batch   900 of 2,000\n",
            "Batch 1,200 of 2,000\n",
            "Batch 1,500 of 2,000\n",
            "Batch 1,800 of 2,000\n",
            "---------- Training model No: 2 ----------\n",
            "---------- Epoch No: 1 ----------\n",
            "Batch   300 of 2,000\n",
            "Batch   600 of 2,000\n",
            "Batch   900 of 2,000\n",
            "Batch 1,200 of 2,000\n",
            "Batch 1,500 of 2,000\n",
            "Batch 1,800 of 2,000\n",
            "---------- Epoch No: 2 ----------\n",
            "Batch   300 of 2,000\n",
            "Batch   600 of 2,000\n",
            "Batch   900 of 2,000\n",
            "Batch 1,200 of 2,000\n",
            "Batch 1,500 of 2,000\n",
            "Batch 1,800 of 2,000\n",
            "---------- Epoch No: 3 ----------\n",
            "Batch   300 of 2,000\n",
            "Batch   600 of 2,000\n",
            "Batch   900 of 2,000\n",
            "Batch 1,200 of 2,000\n",
            "Batch 1,500 of 2,000\n",
            "Batch 1,800 of 2,000\n",
            "---------- Epoch No: 4 ----------\n",
            "Batch   300 of 2,000\n",
            "Batch   600 of 2,000\n",
            "Batch   900 of 2,000\n",
            "Batch 1,200 of 2,000\n",
            "Batch 1,500 of 2,000\n",
            "Batch 1,800 of 2,000\n",
            "---------- Epoch No: 5 ----------\n",
            "Batch   300 of 2,000\n",
            "Batch   600 of 2,000\n",
            "Batch   900 of 2,000\n",
            "Batch 1,200 of 2,000\n",
            "Batch 1,500 of 2,000\n",
            "Batch 1,800 of 2,000\n",
            "---------- Training model No: 3 ----------\n",
            "---------- Epoch No: 1 ----------\n",
            "Batch   300 of 2,000\n",
            "Batch   600 of 2,000\n",
            "Batch   900 of 2,000\n",
            "Batch 1,200 of 2,000\n",
            "Batch 1,500 of 2,000\n",
            "Batch 1,800 of 2,000\n",
            "---------- Epoch No: 2 ----------\n",
            "Batch   300 of 2,000\n",
            "Batch   600 of 2,000\n",
            "Batch   900 of 2,000\n",
            "Batch 1,200 of 2,000\n",
            "Batch 1,500 of 2,000\n",
            "Batch 1,800 of 2,000\n",
            "---------- Epoch No: 3 ----------\n",
            "Batch   300 of 2,000\n",
            "Batch   600 of 2,000\n",
            "Batch   900 of 2,000\n",
            "Batch 1,200 of 2,000\n",
            "Batch 1,500 of 2,000\n",
            "Batch 1,800 of 2,000\n",
            "---------- Epoch No: 4 ----------\n",
            "Batch   300 of 2,000\n",
            "Batch   600 of 2,000\n",
            "Batch   900 of 2,000\n",
            "Batch 1,200 of 2,000\n",
            "Batch 1,500 of 2,000\n",
            "Batch 1,800 of 2,000\n",
            "---------- Epoch No: 5 ----------\n",
            "Batch   300 of 2,000\n",
            "Batch   600 of 2,000\n",
            "Batch   900 of 2,000\n",
            "Batch 1,200 of 2,000\n",
            "Batch 1,500 of 2,000\n",
            "Batch 1,800 of 2,000\n",
            "---------- Training model No: 4 ----------\n",
            "---------- Epoch No: 1 ----------\n",
            "Batch   300 of 2,000\n",
            "Batch   600 of 2,000\n",
            "Batch   900 of 2,000\n",
            "Batch 1,200 of 2,000\n",
            "Batch 1,500 of 2,000\n",
            "Batch 1,800 of 2,000\n",
            "---------- Epoch No: 2 ----------\n",
            "Batch   300 of 2,000\n",
            "Batch   600 of 2,000\n",
            "Batch   900 of 2,000\n",
            "Batch 1,200 of 2,000\n",
            "Batch 1,500 of 2,000\n",
            "Batch 1,800 of 2,000\n",
            "---------- Epoch No: 3 ----------\n",
            "Batch   300 of 2,000\n",
            "Batch   600 of 2,000\n",
            "Batch   900 of 2,000\n",
            "Batch 1,200 of 2,000\n",
            "Batch 1,500 of 2,000\n",
            "Batch 1,800 of 2,000\n",
            "---------- Epoch No: 4 ----------\n",
            "Batch   300 of 2,000\n",
            "Batch   600 of 2,000\n",
            "Batch   900 of 2,000\n",
            "Batch 1,200 of 2,000\n",
            "Batch 1,500 of 2,000\n",
            "Batch 1,800 of 2,000\n",
            "---------- Epoch No: 5 ----------\n",
            "Batch   300 of 2,000\n",
            "Batch   600 of 2,000\n",
            "Batch   900 of 2,000\n",
            "Batch 1,200 of 2,000\n",
            "Batch 1,500 of 2,000\n",
            "Batch 1,800 of 2,000\n",
            "---------- Training model No: 5 ----------\n",
            "---------- Epoch No: 1 ----------\n",
            "Batch   300 of 2,000\n",
            "Batch   600 of 2,000\n",
            "Batch   900 of 2,000\n",
            "Batch 1,200 of 2,000\n",
            "Batch 1,500 of 2,000\n",
            "Batch 1,800 of 2,000\n",
            "---------- Epoch No: 2 ----------\n",
            "Batch   300 of 2,000\n",
            "Batch   600 of 2,000\n",
            "Batch   900 of 2,000\n",
            "Batch 1,200 of 2,000\n",
            "Batch 1,500 of 2,000\n",
            "Batch 1,800 of 2,000\n",
            "---------- Epoch No: 3 ----------\n",
            "Batch   300 of 2,000\n",
            "Batch   600 of 2,000\n",
            "Batch   900 of 2,000\n",
            "Batch 1,200 of 2,000\n",
            "Batch 1,500 of 2,000\n",
            "Batch 1,800 of 2,000\n",
            "---------- Epoch No: 4 ----------\n",
            "Batch   300 of 2,000\n",
            "Batch   600 of 2,000\n",
            "Batch   900 of 2,000\n",
            "Batch 1,200 of 2,000\n",
            "Batch 1,500 of 2,000\n",
            "Batch 1,800 of 2,000\n",
            "---------- Epoch No: 5 ----------\n",
            "Batch   300 of 2,000\n",
            "Batch   600 of 2,000\n",
            "Batch   900 of 2,000\n",
            "Batch 1,200 of 2,000\n",
            "Batch 1,500 of 2,000\n",
            "Batch 1,800 of 2,000\n",
            "---------- Training model No: 6 ----------\n",
            "---------- Epoch No: 1 ----------\n",
            "Batch   300 of 2,000\n",
            "Batch   600 of 2,000\n",
            "Batch   900 of 2,000\n",
            "Batch 1,200 of 2,000\n",
            "Batch 1,500 of 2,000\n",
            "Batch 1,800 of 2,000\n",
            "---------- Epoch No: 2 ----------\n",
            "Batch   300 of 2,000\n",
            "Batch   600 of 2,000\n",
            "Batch   900 of 2,000\n",
            "Batch 1,200 of 2,000\n",
            "Batch 1,500 of 2,000\n",
            "Batch 1,800 of 2,000\n",
            "---------- Epoch No: 3 ----------\n",
            "Batch   300 of 2,000\n",
            "Batch   600 of 2,000\n",
            "Batch   900 of 2,000\n",
            "Batch 1,200 of 2,000\n",
            "Batch 1,500 of 2,000\n",
            "Batch 1,800 of 2,000\n",
            "---------- Epoch No: 4 ----------\n",
            "Batch   300 of 2,000\n",
            "Batch   600 of 2,000\n",
            "Batch   900 of 2,000\n",
            "Batch 1,200 of 2,000\n",
            "Batch 1,500 of 2,000\n",
            "Batch 1,800 of 2,000\n",
            "---------- Epoch No: 5 ----------\n",
            "Batch   300 of 2,000\n",
            "Batch   600 of 2,000\n",
            "Batch   900 of 2,000\n",
            "Batch 1,200 of 2,000\n",
            "Batch 1,500 of 2,000\n",
            "Batch 1,800 of 2,000\n",
            "---------- Evaluating model No: 1 ----------\n",
            "Batch   300 of   750\n",
            "Batch   600 of   750\n",
            "0.869\n",
            "---------- Evaluating model No: 2 ----------\n",
            "Batch   300 of   750\n",
            "Batch   600 of   750\n",
            "0.869\n",
            "---------- Evaluating model No: 3 ----------\n",
            "Batch   300 of   750\n",
            "Batch   600 of   750\n",
            "0.869\n",
            "---------- Evaluating model No: 4 ----------\n",
            "Batch   300 of   750\n",
            "Batch   600 of   750\n",
            "0.869\n",
            "---------- Evaluating model No: 5 ----------\n",
            "Batch   300 of   750\n",
            "Batch   600 of   750\n",
            "0.869\n",
            "---------- Evaluating model No: 6 ----------\n",
            "Batch   300 of   750\n",
            "Batch   600 of   750\n",
            "0.869\n",
            "---------- Evaluating global model ----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch   300 of   750\n",
            "Batch   600 of   750\n",
            "0.869\n",
            "---------- Starting cycle 3 ----------\n",
            "---------- Training model No: 1 ----------\n",
            "---------- Epoch No: 1 ----------\n",
            "Batch   300 of 2,000\n",
            "Batch   600 of 2,000\n",
            "Batch   900 of 2,000\n",
            "Batch 1,200 of 2,000\n",
            "Batch 1,500 of 2,000\n",
            "Batch 1,800 of 2,000\n",
            "---------- Epoch No: 2 ----------\n",
            "Batch   300 of 2,000\n",
            "Batch   600 of 2,000\n",
            "Batch   900 of 2,000\n",
            "Batch 1,200 of 2,000\n",
            "Batch 1,500 of 2,000\n",
            "Batch 1,800 of 2,000\n",
            "---------- Epoch No: 3 ----------\n",
            "Batch   300 of 2,000\n",
            "Batch   600 of 2,000\n",
            "Batch   900 of 2,000\n",
            "Batch 1,200 of 2,000\n",
            "Batch 1,500 of 2,000\n",
            "Batch 1,800 of 2,000\n",
            "---------- Epoch No: 4 ----------\n",
            "Batch   300 of 2,000\n",
            "Batch   600 of 2,000\n",
            "Batch   900 of 2,000\n",
            "Batch 1,200 of 2,000\n",
            "Batch 1,500 of 2,000\n",
            "Batch 1,800 of 2,000\n",
            "---------- Epoch No: 5 ----------\n",
            "Batch   300 of 2,000\n",
            "Batch   600 of 2,000\n",
            "Batch   900 of 2,000\n",
            "Batch 1,200 of 2,000\n",
            "Batch 1,500 of 2,000\n",
            "Batch 1,800 of 2,000\n",
            "---------- Training model No: 2 ----------\n",
            "---------- Epoch No: 1 ----------\n",
            "Batch   300 of 2,000\n",
            "Batch   600 of 2,000\n",
            "Batch   900 of 2,000\n",
            "Batch 1,200 of 2,000\n",
            "Batch 1,500 of 2,000\n",
            "Batch 1,800 of 2,000\n",
            "---------- Epoch No: 2 ----------\n",
            "Batch   300 of 2,000\n",
            "Batch   600 of 2,000\n",
            "Batch   900 of 2,000\n",
            "Batch 1,200 of 2,000\n",
            "Batch 1,500 of 2,000\n",
            "Batch 1,800 of 2,000\n",
            "---------- Epoch No: 3 ----------\n",
            "Batch   300 of 2,000\n",
            "Batch   600 of 2,000\n",
            "Batch   900 of 2,000\n",
            "Batch 1,200 of 2,000\n",
            "Batch 1,500 of 2,000\n",
            "Batch 1,800 of 2,000\n",
            "---------- Epoch No: 4 ----------\n",
            "Batch   300 of 2,000\n",
            "Batch   600 of 2,000\n",
            "Batch   900 of 2,000\n",
            "Batch 1,200 of 2,000\n",
            "Batch 1,500 of 2,000\n",
            "Batch 1,800 of 2,000\n",
            "---------- Epoch No: 5 ----------\n",
            "Batch   300 of 2,000\n",
            "Batch   600 of 2,000\n",
            "Batch   900 of 2,000\n",
            "Batch 1,200 of 2,000\n",
            "Batch 1,500 of 2,000\n",
            "Batch 1,800 of 2,000\n",
            "---------- Training model No: 3 ----------\n",
            "---------- Epoch No: 1 ----------\n",
            "Batch   300 of 2,000\n",
            "Batch   600 of 2,000\n",
            "Batch   900 of 2,000\n",
            "Batch 1,200 of 2,000\n",
            "Batch 1,500 of 2,000\n",
            "Batch 1,800 of 2,000\n",
            "---------- Epoch No: 2 ----------\n",
            "Batch   300 of 2,000\n",
            "Batch   600 of 2,000\n",
            "Batch   900 of 2,000\n",
            "Batch 1,200 of 2,000\n",
            "Batch 1,500 of 2,000\n",
            "Batch 1,800 of 2,000\n",
            "---------- Epoch No: 3 ----------\n",
            "Batch   300 of 2,000\n",
            "Batch   600 of 2,000\n",
            "Batch   900 of 2,000\n",
            "Batch 1,200 of 2,000\n",
            "Batch 1,500 of 2,000\n",
            "Batch 1,800 of 2,000\n",
            "---------- Epoch No: 4 ----------\n",
            "Batch   300 of 2,000\n",
            "Batch   600 of 2,000\n",
            "Batch   900 of 2,000\n",
            "Batch 1,200 of 2,000\n",
            "Batch 1,500 of 2,000\n",
            "Batch 1,800 of 2,000\n",
            "---------- Epoch No: 5 ----------\n",
            "Batch   300 of 2,000\n",
            "Batch   600 of 2,000\n",
            "Batch   900 of 2,000\n",
            "Batch 1,200 of 2,000\n",
            "Batch 1,500 of 2,000\n",
            "Batch 1,800 of 2,000\n",
            "---------- Training model No: 4 ----------\n",
            "---------- Epoch No: 1 ----------\n",
            "Batch   300 of 2,000\n",
            "Batch   600 of 2,000\n",
            "Batch   900 of 2,000\n",
            "Batch 1,200 of 2,000\n",
            "Batch 1,500 of 2,000\n",
            "Batch 1,800 of 2,000\n",
            "---------- Epoch No: 2 ----------\n",
            "Batch   300 of 2,000\n",
            "Batch   600 of 2,000\n",
            "Batch   900 of 2,000\n",
            "Batch 1,200 of 2,000\n",
            "Batch 1,500 of 2,000\n",
            "Batch 1,800 of 2,000\n",
            "---------- Epoch No: 3 ----------\n",
            "Batch   300 of 2,000\n",
            "Batch   600 of 2,000\n",
            "Batch   900 of 2,000\n",
            "Batch 1,200 of 2,000\n",
            "Batch 1,500 of 2,000\n",
            "Batch 1,800 of 2,000\n",
            "---------- Epoch No: 4 ----------\n",
            "Batch   300 of 2,000\n",
            "Batch   600 of 2,000\n",
            "Batch   900 of 2,000\n",
            "Batch 1,200 of 2,000\n",
            "Batch 1,500 of 2,000\n",
            "Batch 1,800 of 2,000\n",
            "---------- Epoch No: 5 ----------\n",
            "Batch   300 of 2,000\n",
            "Batch   600 of 2,000\n",
            "Batch   900 of 2,000\n",
            "Batch 1,200 of 2,000\n",
            "Batch 1,500 of 2,000\n",
            "Batch 1,800 of 2,000\n",
            "---------- Training model No: 5 ----------\n",
            "---------- Epoch No: 1 ----------\n",
            "Batch   300 of 2,000\n",
            "Batch   600 of 2,000\n",
            "Batch   900 of 2,000\n",
            "Batch 1,200 of 2,000\n",
            "Batch 1,500 of 2,000\n",
            "Batch 1,800 of 2,000\n",
            "---------- Epoch No: 2 ----------\n",
            "Batch   300 of 2,000\n",
            "Batch   600 of 2,000\n",
            "Batch   900 of 2,000\n",
            "Batch 1,200 of 2,000\n",
            "Batch 1,500 of 2,000\n",
            "Batch 1,800 of 2,000\n",
            "---------- Epoch No: 3 ----------\n",
            "Batch   300 of 2,000\n",
            "Batch   600 of 2,000\n",
            "Batch   900 of 2,000\n",
            "Batch 1,200 of 2,000\n",
            "Batch 1,500 of 2,000\n",
            "Batch 1,800 of 2,000\n",
            "---------- Epoch No: 4 ----------\n",
            "Batch   300 of 2,000\n",
            "Batch   600 of 2,000\n",
            "Batch   900 of 2,000\n",
            "Batch 1,200 of 2,000\n",
            "Batch 1,500 of 2,000\n",
            "Batch 1,800 of 2,000\n",
            "---------- Epoch No: 5 ----------\n",
            "Batch   300 of 2,000\n",
            "Batch   600 of 2,000\n",
            "Batch   900 of 2,000\n",
            "Batch 1,200 of 2,000\n",
            "Batch 1,500 of 2,000\n",
            "Batch 1,800 of 2,000\n",
            "---------- Training model No: 6 ----------\n",
            "---------- Epoch No: 1 ----------\n",
            "Batch   300 of 2,000\n",
            "Batch   600 of 2,000\n",
            "Batch   900 of 2,000\n",
            "Batch 1,200 of 2,000\n",
            "Batch 1,500 of 2,000\n",
            "Batch 1,800 of 2,000\n",
            "---------- Epoch No: 2 ----------\n",
            "Batch   300 of 2,000\n",
            "Batch   600 of 2,000\n",
            "Batch   900 of 2,000\n",
            "Batch 1,200 of 2,000\n",
            "Batch 1,500 of 2,000\n",
            "Batch 1,800 of 2,000\n",
            "---------- Epoch No: 3 ----------\n",
            "Batch   300 of 2,000\n",
            "Batch   600 of 2,000\n",
            "Batch   900 of 2,000\n",
            "Batch 1,200 of 2,000\n",
            "Batch 1,500 of 2,000\n",
            "Batch 1,800 of 2,000\n",
            "---------- Epoch No: 4 ----------\n",
            "Batch   300 of 2,000\n",
            "Batch   600 of 2,000\n",
            "Batch   900 of 2,000\n",
            "Batch 1,200 of 2,000\n",
            "Batch 1,500 of 2,000\n",
            "Batch 1,800 of 2,000\n",
            "---------- Epoch No: 5 ----------\n",
            "Batch   300 of 2,000\n",
            "Batch   600 of 2,000\n",
            "Batch   900 of 2,000\n",
            "Batch 1,200 of 2,000\n",
            "Batch 1,500 of 2,000\n",
            "Batch 1,800 of 2,000\n",
            "---------- Evaluating model No: 1 ----------\n",
            "Batch   300 of   750\n",
            "Batch   600 of   750\n",
            "0.869\n",
            "---------- Evaluating model No: 2 ----------\n",
            "Batch   300 of   750\n",
            "Batch   600 of   750\n",
            "0.869\n",
            "---------- Evaluating model No: 3 ----------\n",
            "Batch   300 of   750\n",
            "Batch   600 of   750\n",
            "0.869\n",
            "---------- Evaluating model No: 4 ----------\n",
            "Batch   300 of   750\n",
            "Batch   600 of   750\n",
            "0.869\n",
            "---------- Evaluating model No: 5 ----------\n",
            "Batch   300 of   750\n",
            "Batch   600 of   750\n",
            "0.869\n",
            "---------- Evaluating model No: 6 ----------\n",
            "Batch   300 of   750\n",
            "Batch   600 of   750\n",
            "0.869\n",
            "---------- Evaluating global model ----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch   300 of   750\n",
            "Batch   600 of   750\n",
            "0.869\n",
            "--- 23300.809284448624 seconds ---\n"
          ]
        }
      ],
      "source": [
        "final_models = train(3,5)\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "FcvAlnxigMxo",
        "p9iNhBG4gVVG",
        "UwjTag4mhDtZ",
        "3_rinTVQgwaw",
        "omE2sWHKn82F",
        "qvilxQ9pq9NQ",
        "Hcg_OwOqsXvC",
        "aCQFuyq0tpln",
        "oupNxfZrFRjJ",
        "o3g-nNwI3zkd"
      ],
      "machine_shape": "hm",
      "name": "Attempt 2 Dec",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}